{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在 PyTorch 中实现字符级 LSTM\n",
    "\n",
    "在此 notebook 中，我将使用 PyTorch 构建一个字符级 LSTM。该网络将根据一段文本逐个字符地接受训练，并逐个字符地生成新的文本。我将用《安娜·卡列尼娜》训练网络。**此模型将能够根据这本小说里的文字生成新的文本。**\n",
    "\n",
    "此网络参考了 Andrej Karpathy 的 [RNN 帖子](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)和 [Torch 中的实现](https://github.com/karpathy/char-rnn)。下面是这个字符级 RNN 的一般结构。\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">\n",
    "\n",
    "首先加载资源，以便加载数据和创建模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据\n",
    "\n",
    "然后，我们将加载《安娜·卡列尼娜》文本文件并将其转换为整数，这样才能使用于网络中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open text file and read in data as `text`\n",
    "with open('data/anna.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1985223"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查前 100 个字符，看看是否一切顺利。根据[美国图书评论](http://americanbookreview.org/100bestlines.asp)，这本小说的开篇第一句是有史以来排名第六的图书名言佳句。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标记化\n",
    "\n",
    "在下面的单元格中，我创建了几个在字符和整数之间进行转换的**字典**。将字符变成整数便于我们将它们当做输入传入网络中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the text and map each character to an integer and vice versa\n",
    "\n",
    "# we create two dictionaries:\n",
    "# 1. int2char, which maps integers to characters\n",
    "# 2. char2int, which maps characters to unique integers\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "# encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到上面的字符变成了整数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([79, 51, 26, 38, 27, 41, 67, 22, 74, 78, 78, 78, 71, 26, 38, 38, 54,\n",
       "       22, 29, 26, 34, 55, 36, 55, 41,  6, 22, 26, 67, 41, 22, 26, 36, 36,\n",
       "       22, 26, 36, 55, 58, 41, 42, 22, 41, 65, 41, 67, 54, 22, 21, 15, 51,\n",
       "       26, 38, 38, 54, 22, 29, 26, 34, 55, 36, 54, 22, 55,  6, 22, 21, 15,\n",
       "       51, 26, 38, 38, 54, 22, 55, 15, 22, 55, 27,  6, 22, 24, 57, 15, 78,\n",
       "       57, 26, 54, 39, 78, 78, 31, 65, 41, 67, 54, 27, 51, 55, 15])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理数据\n",
    "\n",
    "从上面的 char-RNN 示意图中可以看出，LSTM 层级要求输入是**独热编码**，意思是（通过创建的字典）将每个字符转换为一个整数，*然后*转换为列向量，只有相应的整数索引值将为 1，向量的其他部分全为 0。因为我们要独热编码数据，所以创建一个函数来执行此操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]]]\n"
     ]
    }
   ],
   "source": [
    "# check that the function works as expected\n",
    "test_seq = np.array([[3, 5, 1]])\n",
    "one_hot = one_hot_encode(test_seq, 8)\n",
    "\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建训练迷你批次\n",
    "\n",
    "\n",
    "要使用此数据训练模型，我们还需要创建迷你批次。我们希望批次是多个序列，由一定数量的序列步组成。下面是一个简单的批次示例：\n",
    "\n",
    "<img src=\"assets/sequence_batching@1x.png\" width=500px>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "在此示例中，我们将获取编码字符（作为 `arr` 参数传入），并根据 `batch_size` 将它们拆分为多个序列。每个序列长为 `seq_length`。\n",
    "\n",
    "### 创建批次\n",
    "\n",
    "**1.首先我们需要丢弃一些文本，从而创建完整的迷你批次。**\n",
    "\n",
    "每个批次包含 $N \\times M$ 个字符，其中 $N$ 是批次大小（一个批次中的序列数量），$M$ 是 seq_length，即一个序列中的时间步数。要获取可以根据数组 `arr` 创建的批次总数 $K$，将 `arr` 的长度除以每个批次的字符数。获取批次数量后，就能算出可以从 `arr` 中保留的字符总数：$N * M * K$。\n",
    "\n",
    "**2.之后，我们需要将 `arr` 拆分为 $N$ 批。** \n",
    "\n",
    "我们可以使用 `arr.reshape(size)`，其中 `size` 是一个元组，包含变形数组的维度大小。我们希望一个批次里有 $N$ 个序列，所以将其设为第一个维度的大小。对于第二个维度，你可以使用 `-1` 作为大小占位符，它将使用合适的数据填充数组。之后就获得一个 $N \\times (M * K)$ 数组。\n",
    "\n",
    "**3.获得此数组后，我们可以遍历它以获取迷你批次。**\n",
    "\n",
    "每个批次是应用在 $N \\times (M * K)$ 数组之上的 $N \\times M$ 窗口。对于每个后续批次，窗口将移动 `seq_length` 个位置。我们还需要创建输入和目标数组。目标是偏移一个字符的输入。对于此窗口，我将通过 `range` 采取一定的步数，步数为 `n_steps`，从 $0$ 到 `arr.shape[1]`，即每个序列中的标记总数。这样的话，从 `range` 获取的整数始终指向一个批次的开头，每个窗口的宽为 `seq_length`。\n",
    "\n",
    "> **TODO：**在下面的函数中编写创建批次的代码。此 notebook 中的练习_不太容易_。除了此 notebook 之外，我还提供了 solution notebook。如果你遇到问题，请参阅 solution notebook。最重要的一点是，不要只复制粘贴代码，而是**自己试着编写代码。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    # total number of batches we can make\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    # iterate through the array, one sequence at a time\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试实现代码\n",
    "\n",
    "现在我将创建一些数据集，看看批处理数据会发生什么。我将批次大小设为 8，序列步数设为 50。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 50) (8, 50)\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(encoded, 8, 50)\n",
    "x, y = next(batches)\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[79 51 26 38 27 41 67 22 74 78]\n",
      " [ 6 24 15 22 27 51 26 27 22 26]\n",
      " [41 15  2 22 24 67 22 26 22 29]\n",
      " [ 6 22 27 51 41 22 75 51 55 41]\n",
      " [22  6 26 57 22 51 41 67 22 27]\n",
      " [75 21  6  6 55 24 15 22 26 15]\n",
      " [22 68 15 15 26 22 51 26  2 22]\n",
      " [80 20 36 24 15  6 58 54 39 22]]\n",
      "\n",
      "y\n",
      " [[51 26 38 27 41 67 22 74 78 78]\n",
      " [24 15 22 27 51 26 27 22 26 27]\n",
      " [15  2 22 24 67 22 26 22 29 24]\n",
      " [22 27 51 41 22 75 51 55 41 29]\n",
      " [ 6 26 57 22 51 41 67 22 27 41]\n",
      " [21  6  6 55 24 15 22 26 15  2]\n",
      " [68 15 15 26 22 51 26  2 22  6]\n",
      " [20 36 24 15  6 58 54 39 22 53]]\n"
     ]
    }
   ],
   "source": [
    "# printing out the first 10 items in a sequence\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你正确地实现了 `get_batches`，上述输出应该如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "x\n",
    " [[25  8 60 11 45 27 28 73  1  2]\n",
    " [17  7 20 73 45  8 60 45 73 60]\n",
    " [27 20 80 73  7 28 73 60 73 65]\n",
    " [17 73 45  8 27 73 66  8 46 27]\n",
    " [73 17 60 12 73  8 27 28 73 45]\n",
    " [66 64 17 17 46  7 20 73 60 20]\n",
    " [73 76 20 20 60 73  8 60 80 73]\n",
    " [47 35 43  7 20 17 24 50 37 73]]\n",
    "\n",
    "y\n",
    " [[ 8 60 11 45 27 28 73  1  2  2]\n",
    " [ 7 20 73 45  8 60 45 73 60 45]\n",
    " [20 80 73  7 28 73 60 73 65  7]\n",
    " [73 45  8 27 73 66  8 46 27 65]\n",
    " [17 60 12 73  8 27 28 73 45 27]\n",
    " [64 17 17 46  7 20 73 60 20 80]\n",
    " [76 20 20 60 73  8 60 80 73 17]\n",
    " [35 43  7 20 17 24 50 37 73 36]]\n",
    " ```\n",
    " 当然实际数字可能有所不同。检查 `y` 的数据是否偏移了一个位置。\n",
    "\n",
    "---\n",
    "## 在 PyTorch 中定义网络\n",
    "\n",
    "你将在下面定义网络。\n",
    "\n",
    "<img src=\"assets/charRNN.png\" width=500px>\n",
    "\n",
    "接下来，你将使用 PyTorch 定义网络结构。首先定义层级和要执行的运算。然后定义前向传递方法。我们还提供了预测字符的方法。\n",
    "\n",
    "### 模型结构\n",
    "\n",
    "在 `__init__` 中，建议的结构如下所示：\n",
    "* 创建并存储必要的字典（已经替你完成这一步）\n",
    "* 定义一个接受以下参数的 LSTM 层级：输入大小（字符数）、隐藏层大小 `n_hidden`、层数 `n_layers`、丢弃概率 `drop_prob`，以及 batch_first 布尔值（设为 True，因为我们要批处理）\n",
    "* 使用 `dropout_prob` 定义丢弃层\n",
    "* 定义一个接受以下参数的全连接层：输入大小 `n_hidden` 和输出大小（字符数）\n",
    "* 最后，初始化权重（已经替你完成这一步）\n",
    "\n",
    "注意，`__init__` 函数已经命名并提供某些参数，我们以 `self.drop_prob = drop_prob` 格式使用并存储这些参数。\n",
    "\n",
    "---\n",
    "### LSTM 输入/输出\n",
    "\n",
    "你可以如下所示地创建基本 [LSTM 层级](https://pytorch.org/docs/stable/nn.html#lstm)\n",
    "\n",
    "```python\n",
    "self.lstm = nn.LSTM(input_size, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中 `input_size` 是此单元希望序列输入具有的字符数，`n_hidden` 是单元中的隐藏层里包含的单元数量。要添加丢弃层，我们可以添加丢弃参数并指定丢弃概率；丢弃参数会自动向输入或输出添加丢弃层。最后，在 `forward` 函数中使用 `.view` 将 LSTM 单元堆叠为层。然后传入单元列表，它会将一个单元的输出传入下个单元。\n",
    "\n",
    "还需要将隐藏状态全初始化为 0。代码如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "self.init_hidden()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU; consider making n_epochs very small.\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        ## TODO: define the LSTM\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        ## TODO: define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## TODO: define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        ## TODO: pass through a dropout layer\n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        # you may need to use contiguous to reshape the output\n",
    "#         out = out.contiguous().view(-1, self.n_hidden)\n",
    "        out = out.permute(1, 0, 2).contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        ## TODO: put x through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型\n",
    "\n",
    "我们将在 train() 函数中设定周期数、学习速率和其他参数。\n",
    "\n",
    "我们在下面使用 Adam 优化器和交叉熵损失，因为输出是字符类别分数。照常计算损失并执行反向传播步骤。\n",
    "\n",
    "关于训练的几个细节信息： \n",
    ">* 我们在训练循环中将隐藏状态与其历史记录分离开；这次将其设为新的*元组*变量，因为 LSTM 有一个隐藏状态，该隐藏状态是由隐藏状态和单元状态组成的元组。\n",
    "* 我们使用[`clip_grad_norm_`](https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html) 防止梯度爆炸。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workspace_utils import keep_awake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "#     for e in range(epochs):\n",
    "    for e in keep_awake(range(epochs)):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例化模型\n",
    "\n",
    "现在可以正式训练网络了。首先创建网络并传入一些超参数。然后定义迷你批次大小，就可以开始训练了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## TODO: set you model hyperparameters\n",
    "# define and print the net\n",
    "n_hidden=512\n",
    "n_layers=2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置训练超参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import sucess\n"
     ]
    }
   ],
   "source": [
    "from workspace_utils import active_session\n",
    "\n",
    "with active_session():\n",
    "    print('import sucess')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5... Step: 10... Loss: 3.2460... Val Loss: 3.1804\n",
      "Epoch: 1/5... Step: 20... Loss: 3.1417... Val Loss: 3.1298\n",
      "Epoch: 1/5... Step: 30... Loss: 3.1371... Val Loss: 3.1228\n",
      "Epoch: 1/5... Step: 40... Loss: 3.1119... Val Loss: 3.1210\n",
      "Epoch: 1/5... Step: 50... Loss: 3.1438... Val Loss: 3.1192\n",
      "Epoch: 1/5... Step: 60... Loss: 3.1210... Val Loss: 3.1183\n",
      "Epoch: 1/5... Step: 70... Loss: 3.1104... Val Loss: 3.1187\n",
      "Epoch: 1/5... Step: 80... Loss: 3.1297... Val Loss: 3.1189\n",
      "Epoch: 1/5... Step: 90... Loss: 3.1308... Val Loss: 3.1189\n",
      "Epoch: 1/5... Step: 100... Loss: 3.1272... Val Loss: 3.1191\n",
      "Epoch: 1/5... Step: 110... Loss: 3.1280... Val Loss: 3.1186\n",
      "Epoch: 1/5... Step: 120... Loss: 3.1114... Val Loss: 3.1182\n",
      "Epoch: 1/5... Step: 130... Loss: 3.1279... Val Loss: 3.1183\n",
      "Epoch: 2/5... Step: 140... Loss: 3.1285... Val Loss: 3.1185\n",
      "Epoch: 2/5... Step: 150... Loss: 3.1300... Val Loss: 3.1182\n",
      "Epoch: 2/5... Step: 160... Loss: 3.1198... Val Loss: 3.1184\n",
      "Epoch: 2/5... Step: 170... Loss: 3.0968... Val Loss: 3.1181\n",
      "Epoch: 2/5... Step: 180... Loss: 3.1065... Val Loss: 3.1189\n",
      "Epoch: 2/5... Step: 190... Loss: 3.1118... Val Loss: 3.1184\n",
      "Epoch: 2/5... Step: 200... Loss: 3.1098... Val Loss: 3.1179\n",
      "Epoch: 2/5... Step: 210... Loss: 3.1171... Val Loss: 3.1187\n",
      "Epoch: 2/5... Step: 220... Loss: 3.1200... Val Loss: 3.1188\n",
      "Epoch: 2/5... Step: 230... Loss: 3.1114... Val Loss: 3.1188\n",
      "Epoch: 2/5... Step: 240... Loss: 3.1106... Val Loss: 3.1191\n",
      "Epoch: 2/5... Step: 250... Loss: 3.1102... Val Loss: 3.1187\n",
      "Epoch: 2/5... Step: 260... Loss: 3.1055... Val Loss: 3.1181\n",
      "Epoch: 2/5... Step: 270... Loss: 3.1055... Val Loss: 3.1186\n",
      "Epoch: 3/5... Step: 280... Loss: 3.1205... Val Loss: 3.1185\n",
      "Epoch: 3/5... Step: 290... Loss: 3.1054... Val Loss: 3.1183\n",
      "Epoch: 3/5... Step: 300... Loss: 3.1230... Val Loss: 3.1185\n",
      "Epoch: 3/5... Step: 310... Loss: 3.1156... Val Loss: 3.1181\n",
      "Epoch: 3/5... Step: 320... Loss: 3.1108... Val Loss: 3.1190\n",
      "Epoch: 3/5... Step: 330... Loss: 3.1009... Val Loss: 3.1187\n",
      "Epoch: 3/5... Step: 340... Loss: 3.0999... Val Loss: 3.1181\n",
      "Epoch: 3/5... Step: 350... Loss: 3.1053... Val Loss: 3.1189\n",
      "Epoch: 3/5... Step: 360... Loss: 3.1026... Val Loss: 3.1189\n",
      "Epoch: 3/5... Step: 370... Loss: 3.1224... Val Loss: 3.1190\n",
      "Epoch: 3/5... Step: 380... Loss: 3.1174... Val Loss: 3.1193\n",
      "Epoch: 3/5... Step: 390... Loss: 3.1166... Val Loss: 3.1187\n",
      "Epoch: 3/5... Step: 400... Loss: 3.1091... Val Loss: 3.1182\n",
      "Epoch: 3/5... Step: 410... Loss: 3.1087... Val Loss: 3.1187\n",
      "Epoch: 4/5... Step: 420... Loss: 3.1016... Val Loss: 3.1187\n",
      "Epoch: 4/5... Step: 430... Loss: 3.1032... Val Loss: 3.1186\n",
      "Epoch: 4/5... Step: 440... Loss: 3.1014... Val Loss: 3.1185\n",
      "Epoch: 4/5... Step: 450... Loss: 3.0817... Val Loss: 3.1183\n",
      "Epoch: 4/5... Step: 460... Loss: 3.1006... Val Loss: 3.1190\n",
      "Epoch: 4/5... Step: 470... Loss: 3.1050... Val Loss: 3.1186\n",
      "Epoch: 4/5... Step: 480... Loss: 3.0972... Val Loss: 3.1182\n",
      "Epoch: 4/5... Step: 490... Loss: 3.1023... Val Loss: 3.1189\n",
      "Epoch: 4/5... Step: 500... Loss: 3.1017... Val Loss: 3.1188\n",
      "Epoch: 4/5... Step: 510... Loss: 3.1162... Val Loss: 3.1189\n",
      "Epoch: 4/5... Step: 520... Loss: 3.1212... Val Loss: 3.1194\n",
      "Epoch: 4/5... Step: 530... Loss: 3.0928... Val Loss: 3.1187\n",
      "Epoch: 4/5... Step: 540... Loss: 3.0917... Val Loss: 3.1181\n",
      "Epoch: 4/5... Step: 550... Loss: 3.1064... Val Loss: 3.1186\n",
      "Epoch: 5/5... Step: 560... Loss: 3.0870... Val Loss: 3.1186\n",
      "Epoch: 5/5... Step: 570... Loss: 3.0952... Val Loss: 3.1185\n",
      "Epoch: 5/5... Step: 580... Loss: 3.0977... Val Loss: 3.1184\n",
      "Epoch: 5/5... Step: 590... Loss: 3.1097... Val Loss: 3.1184\n",
      "Epoch: 5/5... Step: 600... Loss: 3.0808... Val Loss: 3.1189\n",
      "Epoch: 5/5... Step: 610... Loss: 3.0998... Val Loss: 3.1186\n",
      "Epoch: 5/5... Step: 620... Loss: 3.0938... Val Loss: 3.1184\n",
      "Epoch: 5/5... Step: 630... Loss: 3.1179... Val Loss: 3.1189\n",
      "Epoch: 5/5... Step: 640... Loss: 3.1119... Val Loss: 3.1188\n",
      "Epoch: 5/5... Step: 650... Loss: 3.1082... Val Loss: 3.1187\n",
      "Epoch: 5/5... Step: 660... Loss: 3.1150... Val Loss: 3.1193\n",
      "Epoch: 5/5... Step: 670... Loss: 3.1269... Val Loss: 3.1187\n",
      "Epoch: 5/5... Step: 680... Loss: 3.0919... Val Loss: 3.1182\n",
      "Epoch: 5/5... Step: 690... Loss: 3.1005... Val Loss: 3.1186\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 5 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "with active_session():\n",
    "    train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获得最佳模型\n",
    "\n",
    "要设置超参数以获得最佳效果，你需要观察训练和验证损失。如果训练损失比验证损失低多了，则表明出现过拟合现象。请添加正则化（更多丢弃层）或使用更小的网络。如果训练和验证损失很接近，则说明网络欠拟合，所以你需要增大网络。\n",
    "\n",
    "## 超参数\n",
    "\n",
    "下面是网络的超参数。\n",
    "\n",
    "在模型定义部分：\n",
    "* `n_hidden` - 隐藏层的单元数量。\n",
    "* `n_layers` - 隐藏 LSTM 层级的数量。\n",
    "\n",
    "在此例中，我们将保留默认的丢弃概率和学习速率。\n",
    "\n",
    "在训练部分：\n",
    "* `batch_size` - 一次经过网络的序列数量。\n",
    "* `seq_length` - 用于训练网络的序列中的字符数。通常网络越大越好，使网络能够学习更多的上下文信息。但是训练时间更长。通常 100 个就很合适。\n",
    "* `lr` - 训练过程中的学习速率\n",
    "\n",
    "下面是 Andrej Karpathy 提出的一些网络训练的好建议。我直接摘录了他的建议，你也可以点击此链接查看[原文](https://github.com/karpathy/char-rnn#tips-and-tricks)。\n",
    "\n",
    "> ## 提示和技巧\n",
    "\n",
    ">### 监控验证损失与训练损失\n",
    ">如果你是机器学习或神经网络新手，那么获取好的模型需要一些技巧。要记录的最重要指标是训练损失（在训练期间输出）和验证损失（当 RNN 在验证数据上运行时，每隔一段时间输出验证损失，默认情况下是每迭代 1000 次）之间的差别。尤其是：\n",
    "\n",
    "> - 如果训练损失比验证损失低得多，则表明网络可能**过拟合**了。解决方案是缩小网络，或增加丢弃率。例如，你可以尝试将丢弃率设为 0.5，等等。\n",
    "> - 如果训练/验证损失差不多大，那么模型**欠拟合**了。请增大模型（可以增加层级数量或每个层级的原始神经元数量）\n",
    "\n",
    "> ### 估计参数值\n",
    "\n",
    "> 控制模型的两个最重要的参数是 `n_hidden` 和 `n_layers`。建议 `n_layers` 始终设为 2 或 3。可以根据数据量调整 `n_hidden`。要记录的两个最重要的指标是：\n",
    "\n",
    "> - 模型中的参数数量。在开始训练时输出该数量。\n",
    "> - 数据集大小。1MB 文件约为 100 万个字符。\n",
    "\n",
    ">这两个应该约为相同的数量级。这些参数比较难设定。下面是一些示例：\n",
    "\n",
    "> - 我有一个 100MB 的数据集，我使用默认的参数设置（当前会输出 15 万个参数）。数据大得多（1 亿远远大于15 万），所以肯定会严重欠拟合。我完全可以增大 `n_hidden`。\n",
    "> - 我有一个 10MB 的数据集，并且模型有 1000 万个参数。也就是说，我们需要仔细监控验证损失。如果大于训练损失，那么就需要稍微增大丢弃概率，看看能否降低验证损失。\n",
    "\n",
    "> ### 最佳模型策略\n",
    "\n",
    ">获得很好模型的最佳策略（如果你有计算时间）是始终尝试更大的模型（大到愿意花费一定的计算时间），然后尝试不同的丢弃值（在 0 到 1 之间）。最终应该使用验证效果最好的模型（写入检查点文件里的损失越低越好）。\n",
    "\n",
    ">在深度学习领域，最常见的做法是用很多不同的超参数设置运行大量不同的模型，最终选择验证损失最佳的检查点。\n",
    "\n",
    ">顺便提下，训练集和验证集划分也是参数。验证集中应该有足够的数据，否则验证效果将有噪点，并且不能提供可靠的信息。\n",
    "\n",
    "## 检查点\n",
    "\n",
    "训练之后，我们将保存模型，方便以后重新加载模型。我将保存创建相同架构所需的参数、隐藏层超参数和文本字符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the name, for saving multiple files\n",
    "model_name = 'rnn_x_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 做出预测\n",
    "\n",
    "训练好模型后，我们需要从中抽样并预测下个字符。为了抽样，我们传入一个字符并使网络预测下个字符。然后将该字符传入网络中并预测下个字符。不断这么操作，直到生成大量文本。\n",
    "\n",
    "### 关于 `predict` 函数的注意事项\n",
    "\n",
    "RNN 的输出来自全连接层，它会输出**下个字符的分数分布**。\n",
    "\n",
    "> 要获得下个字符，我们应用 softmax 函数，它会提供*概率*分布，我们然后从中抽样并预测下个字符。\n",
    "\n",
    "### Top-K 抽样\n",
    "\n",
    "预测来自所有潜在字符的类别概率分布。我们可以抽样文本并仅考虑前 $K$ 个潜在字符，使抽样文本更合理（变量更少）。这样可以避免网络提供完全不合理的字符，并且能够向抽样文本里引入一些噪点和随机性。详细了解 [top-k](https://pytorch.org/docs/stable/torch.html#torch.topk)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设定 prime 单词并生成文本 \n",
    "\n",
    "通常，需要设定 prime 单词来构建隐藏状态。否则，网络将开始随机生成字符。前几个字符一般比较难预测，因为预测上下文信息不足。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annatoo oe tooee ettao   e  eee oaoete e  ooete  eoe o oottoeaeotte attoeoe aeaa e   o   o oe  a   e  aoe toa  eeeteeaetaeo ta tea tta   o oeet ottoatt  t tt   aaet ooo e  te ot eee  eotttaeo taeo tta eetee  aoeoa   e eaeaotoae a  eto e otea t ttee atooe e  eoo       etoe   toeoeeatta  ottoeaatee    otee eat t  toeato  ato  eeto tte  eeaoeoa  aaeeototaet t    o   e e      at aa  eteet   eet eaaea   eettt a oa  toa ao tta te     t  oa eattt  eo teae  tto eeaeeoaeo aetae   o e  ee    et te e  o teoo a eea tte a e t     ooteaooetoo e t oteto teatoteee tee ea o o ee tooa ae   oetae t ee aatoe ooa e ea  t at o ao t et o  o    o tae oo aa  atatoe at e  ooee teete eto oe  e  e a te    oa ae   aaea  a ttte   oaa ta   ata ta ao t o a   ee  ooto t  e etatta e oaoe oo  eaea oo aatt      et etattteeo tete   o ao  taeat oaoe a o  o e oaaoe eetee ae  oe oeo ete ao    o  ee ett  eeeeaeaoeote et o eaetao o     aate etoet tt  oteaa   e    toao  e e teate  ooo atooa e    oe  oeeoe tooeeet t a    o  o  aa e  \n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime='Anna', top_k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载检查点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
    "with open('rnn_x_epoch.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And Levin saidoettaa   aettteaaeteett oo  o o t tt eeeaot  aoeetet  aoao  t to e  oe  e  eeoaae  ttaaae   et o    oe a a a atat oaoo     t  tt  o eata aa ooo t etaa  e e e   a   ee    e  eto  t et e eta   t  oe aae oo et  eao  e  a e ot teeot   eo ee  t  et    t otet  tt t eaa  t  aa    a a  aete t  oo t  a  oeee  ate    otet   ea a  at oat oetoto t  tooa ooettteae  te  a   a   ea t  ae  ottt  e  o  eeaeatoott e eta e  e  ae ae  et etaetoteoaeeet tetata t eteaoaataeet otot       e  ot ea aeaeao  a e e toa t toa aeee e  ooeea t ae  ot teoee e eoeeott  eaetot oo tt t eoott a oa  oa et e taae   eo   aeooae  eeooao   a aa e ota eeoa  eaet attaat  oeeo  e  ttoa   aeo   taet  eooae eoattaaote t  ee ee  aotataoaettaa ao ttet e  t eo teett o   oa ete e o eo oeeeea  a et eaooot a   a  eaa  aeeete t ato  e  a o e too toeeo  eaoeeaaaaa oooaoeoo     a  e  e  t  e   tea oe  oatea otett et otoee  t  eo a   ee e  o eeaot  etee ae ea  teo ae    otteoee  o aaee o a tt a eea t o   oao t   aaee  ao t ao    aoeeeeo  tate   tetetae     otte   e ete   atet to eeat  ao oao  o taoae e  oeo oae aeet eoee toeaea eoaoae e aaoot t ttaa eeeo e oeetaeeeo  te eotoeoe o oat  o e ataoe e otaea t oe  a e ao  t  ot oeeo  o  te ee a  to aae ttteaeeae  oeoea  te teote o oo ta oot  a e o otta eoe eoe  oaoe      tte  eet at oet t tota te tteaa      oa e  eta eee ee  aaa ee   e     a t a  ae    eat t ea atote eeeo   oa aea  ea aa e e eetotee  t  eot  a a aee    etaeteto  eaoaa    a a eo tattoettte   tao teeet tae aa  ea ea taee ee to oaoo  o oato   oeta eoo eoaataaeoet e eeete tee  oata  t o tett  et  at t  ot eaa ttoeaea taoe a e eae t e  e o o    ae   oo ate aa t oe o e  e tatteoao aete eeat  atoee aae ta t teo   et oaa  a et     tataoaaetoet e ee oaot ee  teee     eottaetet ta aoatee  te t ae  e aae aa taeotoott  oa a ooe ee t oe   tt eet a oe ettao  o  oaeeetae atoee at  oeea eoeteao    eat ta   et oataet oeaeaee ee t ot   taeea ee e t eetee  t o eateaea   t ttoooto  toeoeo atet teeet e eta a   et eo e   eott oa too\n"
     ]
    }
   ],
   "source": [
    "# Sample using a loaded model\n",
    "print(sample(loaded, 2000, top_k=5, prime=\"And Levin said\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
