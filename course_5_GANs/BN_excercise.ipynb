{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization – Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "批量正则化在构建深层神经网络时最有用。为了证明这一点，我们将创建一个包含<u>**20个卷积层**</u>的卷积神经网络，然后是<u>**1个全连接层**</u>。我们将使用它对*MNIST*数据集中的手写数字进行分类，您现在应该已经熟悉了。         \n",
    "          \n",
    "这**不是**一个很好的网络分类*MINIST*数字。你可以创建一个**简单**网络，得到**更好**结果。但是，为了让您亲身体验批量正则化，我们必须举一个例子：   \n",
    "1.足够复杂，训练将受益于批量正则化。     \n",
    "2.简单到可以快速训练，因为这是一个简短的练习，只是为了给您提供一些添加批量正则化的实践。        \n",
    "3.简单到不需要额外的资源就能很容易理解体系结构。     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此笔记本包含两个可以编辑的神经网络版本。第一个使用来自`tf.layers`包的高级函数。第二个是相同的网络，但只使用`tf.nn`包中的低级函数。\n",
    "\n",
    "1. [Batch Normalization with `tf.layers.batch_normalization`](#example_1)\n",
    "2. [Batch Normalization with `tf.nn.batch_normalization`](#example_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下单元格加载TensorFlow，必要时下载*MNIST*数据集，并将其加载到名为`MNIST`的对象中。在笔记本上运行任何其他内容之前，您需要运行此单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "CPU times: user 13.6 s, sys: 830 ms, total: 14.4 s\n",
      "Wall time: 14.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True, reshape=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization using `tf.layers.batch_normalization`<a id=\"example_1\"></a>\n",
    "\n",
    "$$\\color{red}{Attention}$$    \n",
    "此版本的网络几乎对所有内容都使用`tf.layers`，并希望您使用[`tf.layers.batch_normalization`](https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization) \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将使用以下函数在网络中创建完全连接的层。我们将用指定数量的神经元和ReLU激活函数创建它们。     \n",
    "         \n",
    "此版本的函数不包括*批量正则化*。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS CELL\n",
    "\"\"\"\n",
    "def fully_connected(prev_layer, num_units):\n",
    "    \"\"\"\n",
    "    Create a fully connectd layer with the given layer as input and the given number of neurons.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param num_units: int\n",
    "        The size of the layer. That is, the number of units, nodes, or neurons.\n",
    "    :returns Tensor\n",
    "        A new fully connected layer\n",
    "    \"\"\"\n",
    "    layer = tf.layers.dense(prev_layer, num_units, activation=tf.nn.relu)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将使用以下函数在网络中创建卷积层。它们是非常基本的：我们总是使用3x3内核，ReLU激活函数，在奇数深度的层上跨步1x1，在偶数深度的层上跨步2x2。在这个网络中，我们根本不需要把层集中起来。        \n",
    "              \n",
    "此版本的函数不包括*批量正则化*。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS CELL\n",
    "\"\"\"\n",
    "def conv_layer(prev_layer, layer_depth):\n",
    "    \"\"\"\n",
    "    Create a convolutional layer with the given layer as input.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param layer_depth: int\n",
    "        We'll set the strides and number of feature maps based on the layer's depth in the network.\n",
    "        This is *not* a good way to make a CNN, but it helps us create this example with very little code.\n",
    "    :returns Tensor\n",
    "        A new convolutional layer\n",
    "    \"\"\"\n",
    "    strides = 2 if layer_depth % 3 == 0 else 1   # (not good)\n",
    "    conv_layer = tf.layers.conv2d(prev_layer, layer_depth*4, 3, strides, 'same', activation=tf.nn.relu)\n",
    "    return conv_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**运行下面的单元格**，以及前面的单元格（以加载数据集并定义必要的函数）。       \n",
    "       \n",
    "这个单元在没有**批量规范化的情况下构建网络**，然后在MNIST数据集上训练它。它在训练过程中定期显示丢失和准确性数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0: Validation loss: 0.69101, Validation accuracy: 0.10700\n",
      "Batch: 25: Training loss: 0.49650, Training accuracy: 0.04688\n",
      "Batch: 50: Training loss: 0.32611, Training accuracy: 0.10938\n",
      "Batch: 75: Training loss: 0.32385, Training accuracy: 0.14062\n",
      "Batch: 100: Validation loss: 0.32515, Validation accuracy: 0.11260\n",
      "Batch: 125: Training loss: 0.32502, Training accuracy: 0.14062\n",
      "Batch: 150: Training loss: 0.32563, Training accuracy: 0.06250\n",
      "Batch: 175: Training loss: 0.32641, Training accuracy: 0.09375\n",
      "Batch: 200: Validation loss: 0.32502, Validation accuracy: 0.11260\n",
      "Batch: 225: Training loss: 0.32480, Training accuracy: 0.07812\n",
      "Batch: 250: Training loss: 0.32697, Training accuracy: 0.07812\n",
      "Batch: 275: Training loss: 0.32777, Training accuracy: 0.06250\n",
      "Batch: 300: Validation loss: 0.32531, Validation accuracy: 0.09760\n",
      "Batch: 325: Training loss: 0.32744, Training accuracy: 0.04688\n",
      "Batch: 350: Training loss: 0.32578, Training accuracy: 0.12500\n",
      "Batch: 375: Training loss: 0.32504, Training accuracy: 0.15625\n",
      "Batch: 400: Validation loss: 0.32560, Validation accuracy: 0.11000\n",
      "Batch: 425: Training loss: 0.32361, Training accuracy: 0.15625\n",
      "Batch: 450: Training loss: 0.32540, Training accuracy: 0.06250\n",
      "Batch: 475: Training loss: 0.32447, Training accuracy: 0.10938\n",
      "Batch: 500: Validation loss: 0.32583, Validation accuracy: 0.11260\n",
      "Batch: 525: Training loss: 0.32563, Training accuracy: 0.01562\n",
      "Batch: 550: Training loss: 0.32218, Training accuracy: 0.12500\n",
      "Batch: 575: Training loss: 0.32557, Training accuracy: 0.06250\n",
      "Batch: 600: Validation loss: 0.32528, Validation accuracy: 0.11260\n",
      "Batch: 625: Training loss: 0.32572, Training accuracy: 0.04688\n",
      "Batch: 650: Training loss: 0.32830, Training accuracy: 0.09375\n",
      "Batch: 675: Training loss: 0.32542, Training accuracy: 0.09375\n",
      "Batch: 700: Validation loss: 0.32545, Validation accuracy: 0.09240\n",
      "Batch: 725: Training loss: 0.32601, Training accuracy: 0.12500\n",
      "Batch: 750: Training loss: 0.32641, Training accuracy: 0.06250\n",
      "Batch: 775: Training loss: 0.32323, Training accuracy: 0.18750\n",
      "Final validation accuracy: 0.11000\n",
      "Final test accuracy: 0.10280\n",
      "Accuracy on 100 samples: 0.15\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS CELL\n",
    "\"\"\"\n",
    "def train(num_batches, batch_size, learning_rate):\n",
    "    # Build placeholders for the input samples and labels \n",
    "    inputs = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    labels = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    # Feed the inputs into a series of 20 convolutional layers \n",
    "    layer = inputs\n",
    "    for layer_i in range(1, 20):\n",
    "        layer = conv_layer(layer, layer_i)\n",
    "\n",
    "    # Flatten the output from the convolutional layers \n",
    "    orig_shape = layer.get_shape().as_list()\n",
    "    layer = tf.reshape(layer, shape=[-1, orig_shape[1] * orig_shape[2] * orig_shape[3]])\n",
    "\n",
    "    # Add one fully connected layer\n",
    "    layer = fully_connected(layer, 100)\n",
    "\n",
    "    # Create the output layer with 1 node for each \n",
    "    logits = tf.layers.dense(layer, 10)\n",
    "    \n",
    "    # Define loss and training operations\n",
    "    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    train_opt = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "    \n",
    "    # Create operations to test accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # Train and test the network\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for batch_i in range(num_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # train this batch\n",
    "            sess.run(train_opt, {inputs: batch_xs, labels: batch_ys})\n",
    "            \n",
    "            # Periodically check the validation or training loss and accuracy\n",
    "            if batch_i % 100 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: mnist.validation.images,\n",
    "                                                              labels: mnist.validation.labels})\n",
    "                print('Batch: {:>4}/ {:>4}: Validation loss: {:>3.5f}, Validation accuracy: {:>3.5f}'\n",
    "                      .format(batch_i,num_batches ,loss, acc))\n",
    "            elif batch_i % 25 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: batch_xs, labels: batch_ys})\n",
    "                print('Batch: {:>4}/ {:>4}: Training loss: {:>3.5f}, Training accuracy: {:>3.5f}'\n",
    "                      .format(batch_i,num_batches, loss, acc))\n",
    "\n",
    "        # At the end, score the final accuracy for both the validation and test sets\n",
    "        acc = sess.run(accuracy, {inputs: mnist.validation.images,\n",
    "                                  labels: mnist.validation.labels})\n",
    "        print('Final validation accuracy: {:>3.5f}'.format(acc))\n",
    "        acc = sess.run(accuracy, {inputs: mnist.test.images,\n",
    "                                  labels: mnist.test.labels})\n",
    "        print('Final test accuracy: {:>3.5f}'.format(acc))\n",
    "        \n",
    "        # Score the first 100 test images individually. This won't work if batch normalization isn't implemented correctly.\n",
    "        correct = 0\n",
    "        for i in range(100):\n",
    "            correct += sess.run(accuracy,feed_dict={inputs: [mnist.test.images[i]],\n",
    "                                                    labels: [mnist.test.labels[i]]})\n",
    "\n",
    "        print(\"Accuracy on 100 samples:\", correct/100)\n",
    "\n",
    "\n",
    "num_batches = 800\n",
    "batch_size = 64\n",
    "learning_rate = 0.002\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    train(num_batches, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了这么多层，这个网络需要很多迭代才能学习。当你完成这800个批次的培训时，你的最终测试和验证准确率可能不会超过10%。（每次都会有所不同，但很可能低于15%。）        \n",
    "       \n",
    "使用批处理规范化，您将能够在相同的批处理数中将同一网络训练到90%以上。      \n",
    "     \n",
    "# 添加批处理规范化\n",
    "我们已经复制了前三个单元格来开始。**编辑这些单元格**以向网络添加批量规范化。对于这个练习，你应该使用[`tf.layers.batch_normalization`](https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization) 来处理大部分数学问题，但需要对网络进行一些其他更改才能集成批处理规范化。您可能需要参考课程笔记本来提醒自己一些重要的事情，例如图形操作需要知道您是否正在执行培训或推理。      \n",
    "        \n",
    "如果你陷入困境，你可以查看`Batch_Normalization_Solutions` 笔记本，看看我们是怎么做的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 修改`fully_connected`以将批处理规范化添加到它创建的完全连接层。如果有帮助，可以随意更改函数的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected(prev_layer, num_units, is_training):\n",
    "    \"\"\"\n",
    "    Create a fully connectd layer with the given layer as input and the given number of neurons.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param num_units: int\n",
    "        The size of the layer. That is, the number of units, nodes, or neurons.\n",
    "    :param is_training: bool or Tensor\n",
    "        Indicates whether or not the network is currently training, which tells the batch normalization\n",
    "        layer whether or not it should update or use its population statistics.\n",
    "    :returns Tensor\n",
    "        A new fully connected layer\n",
    "    \"\"\"\n",
    "    layer = tf.layers.dense(prev_layer, num_units, use_bias=False, activation=None)\n",
    "    layer = tf.layers.batch_normalization(layer, training=is_training)\n",
    "    layer = tf.nn.relu(layer)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 修改`conv_layer`，将批处理规范化添加到它创建的卷积层。如果有帮助，可以随意更改函数的参数。\n",
    "\n",
    "1.**在标准化之前不使用偏差且在使用ReLU激活函数前添加批量规范化的备用解决方案。** <font color=blue>conv2d -> bn -> relu</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def conv_layer(prev_layer, layer_depth, is_training):\n",
    "#     \"\"\"\n",
    "#     Create a convolutional layer with the given layer as input.\n",
    "    \n",
    "#     :param prev_layer: Tensor\n",
    "#         The Tensor that acts as input into this layer\n",
    "#     :param layer_depth: int\n",
    "#         We'll set the strides and number of feature maps based on the layer's depth in the network.\n",
    "#         This is *not* a good way to make a CNN, but it helps us create this example with very little code.\n",
    "#     :param is_training: bool or Tensor\n",
    "#         Indicates whether or not the network is currently training, which tells the batch normalization\n",
    "#         layer whether or not it should update or use its population statistics.\n",
    "#     :returns Tensor\n",
    "#         A new convolutional layer\n",
    "#     \"\"\"\n",
    "#     strides = 2 if layer_depth % 3 == 0 else 1             # \n",
    "#     conv_layer = tf.layers.conv2d(prev_layer, layer_depth*4, 3, strides, 'same', use_bias=False, activation=None)\n",
    "#     conv_layer = tf.layers.batch_normalization(conv_layer, training=is_training)\n",
    "#     conv_layer = tf.nn.relu(conv_layer)\n",
    "\n",
    "#     return conv_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.**在卷积层中使用偏差但仍在ReLU激活函数之前添加批量规范化的替代解决方案:**\n",
    "<font color=blue>conv2d(bias) -> bn -> relu</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(prev_layer, layer_num, is_training):\n",
    "    \"\"\"\n",
    "    Create a convolutional layer with the given layer as input.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param layer_depth: int\n",
    "        We'll set the strides and number of feature maps based on the layer's depth in the network.\n",
    "        This is *not* a good way to make a CNN, but it helps us create this example with very little code.\n",
    "    :param is_training: bool or Tensor\n",
    "        Indicates whether or not the network is currently training, which tells the batch normalization\n",
    "        layer whether or not it should update or use its population statistics.\n",
    "    :returns Tensor\n",
    "        A new convolutional layer\n",
    "    \"\"\"\n",
    "    strides = 2 if layer_num % 3 == 0 else 1\n",
    "    conv_layer = tf.layers.conv2d(prev_layer, layer_num*4, 3, strides, 'same', use_bias=True, activation=None)\n",
    "    conv_layer = tf.layers.batch_normalization(conv_layer, training=is_training)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    return conv_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.**在批处理规范化之前使用bias和ReLU激活函数的备用解决方案:** <font color=blue>conv2d(bias) -> relu -> bn</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def conv_layer(prev_layer, layer_num, is_training):\n",
    "#     \"\"\"\n",
    "#     Create a convolutional layer with the given layer as input.\n",
    "    \n",
    "#     :param prev_layer: Tensor\n",
    "#         The Tensor that acts as input into this layer\n",
    "#     :param layer_depth: int\n",
    "#         We'll set the strides and number of feature maps based on the layer's depth in the network.\n",
    "#         This is *not* a good way to make a CNN, but it helps us create this example with very little code.\n",
    "#     :param is_training: bool or Tensor\n",
    "#         Indicates whether or not the network is currently training, which tells the batch normalization\n",
    "#         layer whether or not it should update or use its population statistics.\n",
    "#     :returns Tensor\n",
    "#         A new convolutional layer\n",
    "#     \"\"\"\n",
    "#     strides = 2 if layer_num % 3 == 0 else 1\n",
    "#     conv_layer = tf.layers.conv2d(prev_layer, layer_num*4, 3, strides, 'same', use_bias=True, activation=tf.nn.relu)\n",
    "#     conv_layer = tf.layers.batch_normalization(conv_layer, training=is_training)\n",
    "#     return conv_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.**在标准化之前使用ReLU激活函数但没有偏差的备用解决方案。** <font color=blue>conv2d -> relu -> bn</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def conv_layer(prev_layer, layer_num, is_training):\n",
    "#     \"\"\"\n",
    "#     Create a convolutional layer with the given layer as input.\n",
    "    \n",
    "#     :param prev_layer: Tensor\n",
    "#         The Tensor that acts as input into this layer\n",
    "#     :param layer_depth: int\n",
    "#         We'll set the strides and number of feature maps based on the layer's depth in the network.\n",
    "#         This is *not* a good way to make a CNN, but it helps us create this example with very little code.\n",
    "#     :param is_training: bool or Tensor\n",
    "#         Indicates whether or not the network is currently training, which tells the batch normalization\n",
    "#         layer whether or not it should update or use its population statistics.\n",
    "#     :returns Tensor\n",
    "#         A new convolutional layer\n",
    "#     \"\"\"\n",
    "#     strides = 2 if layer_num % 3 == 0 else 1\n",
    "#     conv_layer = tf.layers.conv2d(prev_layer, layer_num*4, 3, strides, 'same', use_bias=False, activation=tf.nn.relu)\n",
    "#     conv_layer = tf.layers.batch_normalization(conv_layer, training=is_training)\n",
    "#     return conv_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\color{red}{Summary}$$\n",
    "\n",
    "批处理规范化仍然是一个足够新的想法，研究人员仍在探索如何最好地使用它。一般来说，人们似乎同意消除层的偏差（因为批处理规范化已经有了缩放和移位的条件），并在层的非线性激活函数之前添加批处理规范化。然而，对于一些网络来说，它在其他方面也会很好地工作。   \n",
    "      \n",
    "为了演示这一点，下面三个版本的conv_layer展示了实现批处理规范化的其他方法。如果尝试使用这些函数的任何版本运行，它们都应该仍然可以正常工作（尽管某些版本可能仍然比其他版本工作得更好）。 \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 编辑`train`函数以支持批量规范化。你需要确保网络知道它是否在训练，并且你需要确保它正确地更新和使用它的人口统计数据。\n",
    "         \n",
    "为了改进`train`，我们做了以下工作：         \n",
    "1.添加了`is_training`，一个用于存储布尔值的占位符，该值指示网络是否正在进行训练。        \n",
    "2.传递`is_training`到`fully_connected` 和 `conv_layer` 函数。          \n",
    "3.每次我们在课程中调用`run`，我们都会将`is_training`的适当值添加到`feed_dict`中。            \n",
    "4.将`train_opt`的创建移动到一个`with tf.control_dependencies...`段落中。这对于获取使用`tf.layers.batch_normalization`创建的规范化层以更新其总体统计数据是必要的，我们在执行推断时需要这些数据。        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0: Validation loss: 0.69115, Validation accuracy: 0.11260\n",
      "Batch: 25: Training loss: 0.58017, Training accuracy: 0.14062\n",
      "Batch: 50: Training loss: 0.46078, Training accuracy: 0.14062\n",
      "Batch: 75: Training loss: 0.39983, Training accuracy: 0.06250\n",
      "Batch: 100: Validation loss: 0.36252, Validation accuracy: 0.09760\n",
      "Batch: 125: Training loss: 0.34913, Training accuracy: 0.12500\n",
      "Batch: 150: Training loss: 0.33554, Training accuracy: 0.09375\n",
      "Batch: 175: Training loss: 0.32542, Training accuracy: 0.23438\n",
      "Batch: 200: Validation loss: 0.28640, Validation accuracy: 0.31160\n",
      "Batch: 225: Training loss: 0.23886, Training accuracy: 0.45312\n",
      "Batch: 250: Training loss: 0.24569, Training accuracy: 0.50000\n",
      "Batch: 275: Training loss: 0.16317, Training accuracy: 0.64062\n",
      "Batch: 300: Validation loss: 0.05708, Validation accuracy: 0.90580\n",
      "Batch: 325: Training loss: 0.04002, Training accuracy: 0.92188\n",
      "Batch: 350: Training loss: 0.09054, Training accuracy: 0.85938\n",
      "Batch: 375: Training loss: 0.06187, Training accuracy: 0.89062\n",
      "Batch: 400: Validation loss: 0.03261, Validation accuracy: 0.95240\n",
      "Batch: 425: Training loss: 0.05274, Training accuracy: 0.92188\n",
      "Batch: 450: Training loss: 0.07092, Training accuracy: 0.85938\n",
      "Batch: 475: Training loss: 0.05814, Training accuracy: 0.85938\n",
      "Batch: 500: Validation loss: 0.03842, Validation accuracy: 0.95200\n",
      "Batch: 525: Training loss: 0.02176, Training accuracy: 0.96875\n",
      "Batch: 550: Training loss: 0.03255, Training accuracy: 0.95312\n",
      "Batch: 575: Training loss: 0.00126, Training accuracy: 1.00000\n",
      "Batch: 600: Validation loss: 0.04030, Validation accuracy: 0.94640\n",
      "Batch: 625: Training loss: 0.04620, Training accuracy: 0.92188\n",
      "Batch: 650: Training loss: 0.01716, Training accuracy: 0.98438\n",
      "Batch: 675: Training loss: 0.04073, Training accuracy: 0.95312\n",
      "Batch: 700: Validation loss: 0.02734, Validation accuracy: 0.96240\n",
      "Batch: 725: Training loss: 0.00969, Training accuracy: 0.98438\n",
      "Batch: 750: Training loss: 0.04614, Training accuracy: 0.93750\n",
      "Batch: 775: Training loss: 0.00558, Training accuracy: 1.00000\n",
      "Final validation accuracy: 0.95980\n",
      "Final test accuracy: 0.95920\n",
      "Accuracy on 100 samples: 0.97\n"
     ]
    }
   ],
   "source": [
    "def train(num_batches, batch_size, learning_rate):\n",
    "    # Build placeholders for the input samples and labels \n",
    "    inputs = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    labels = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "    '''Add placeholder to indicate whether or not we're training the model'''\n",
    "    is_training = tf.placeholder(tf.bool) \n",
    "\n",
    "    # Feed the inputs into a series of 20 convolutional layers \n",
    "    layer = inputs\n",
    "    for layer_i in range(1, 20):\n",
    "        layer = conv_layer(layer, layer_i, is_training)\n",
    "\n",
    "    # Flatten the output from the convolutional layers \n",
    "    orig_shape = layer.get_shape().as_list()\n",
    "    layer = tf.reshape(layer, shape=[-1, orig_shape[1] * orig_shape[2] * orig_shape[3]])\n",
    "\n",
    "    # Add one fully connected layer\n",
    "    layer = fully_connected(layer, 100, is_training)\n",
    "\n",
    "    # Create the output layer with 1 node for each \n",
    "    logits = tf.layers.dense(layer, 10)\n",
    "    \n",
    "    # Define loss and training operations\n",
    "    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    \n",
    "    '''Tell TensorFlow to update the population statistics while training'''\n",
    "    # Wrapper for `Graph.control_dependencies()` using the default graph.\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): \n",
    "        train_opt = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "    \n",
    "    # Create operations to test accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # Train and test the network\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for batch_i in range(num_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # train this batch\n",
    "            sess.run(train_opt, {inputs: batch_xs, labels: batch_ys, is_training: True})\n",
    "            \n",
    "            # Periodically check the validation or training loss and accuracy\n",
    "            if batch_i % 100 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: mnist.validation.images,\n",
    "                                                              labels: mnist.validation.labels,\n",
    "                                                              is_training: False})\n",
    "                print('Batch: {:>2}: Validation loss: {:>3.5f}, Validation accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "            elif batch_i % 25 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: batch_xs, labels: batch_ys, is_training: False})\n",
    "                print('Batch: {:>2}: Training loss: {:>3.5f}, Training accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "\n",
    "        # At the end, score the final accuracy for both the validation and test sets\n",
    "        acc = sess.run(accuracy, {inputs: mnist.validation.images,\n",
    "                                  labels: mnist.validation.labels, \n",
    "                                  is_training: False})\n",
    "        print('Final validation accuracy: {:>3.5f}'.format(acc))\n",
    "        acc = sess.run(accuracy, {inputs: mnist.test.images,\n",
    "                                  labels: mnist.test.labels,\n",
    "                                  is_training: False})\n",
    "        print('Final test accuracy: {:>3.5f}'.format(acc))\n",
    "        \n",
    "        '''Score the first 100 test images individually. This won't work if batch normalization isn't implemented correctly.'''\n",
    "        correct = 0\n",
    "        for i in range(100):\n",
    "            correct += sess.run(accuracy,feed_dict={inputs: [mnist.test.images[i]],\n",
    "                                                    labels: [mnist.test.labels[i]],\n",
    "                                                    is_training: False})\n",
    "\n",
    "        print(\"Accuracy on 100 samples:\", correct/100)\n",
    "\n",
    "\n",
    "num_batches = 800\n",
    "batch_size = 64\n",
    "learning_rate = 0.002\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    train(num_batches, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用批处理规范化，您现在应该可以获得超过90%的精度。还要注意输出的最后一行：`Accuracy on 100 samples`。如果此值较低，而其他值看起来都很好，则意味着您没有正确实现批处理规范化。具体来说，这意味着你要么**在训练时没有计算总体均值和方差**，要么**在推理时没有使用这些值**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization using `tf.nn.batch_normalization`<a id=\"example_2\"></a>\n",
    "\n",
    "大多数情况下，您可以专门使用较高级别的函数，但有时您可能希望在较低级别工作。例如，如果您想要实现一个新特性（某个新特性使得TensorFlow还没有包含它的高级实现，比如LSTM中的批处理规范化），那么您可能需要知道这些事情。    \n",
    "         \n",
    "此版本的网络几乎对所有内容都使用`tf.nn`，并希望您使用[`tf.nn.batch_normalization`](https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization).\n",
    "\n",
    "**optional TODO:** 您可以在编辑前运行接下来的三个单元格，以查看网络在没有批处理规范化的情况下的性能。但是，结果应该与添加批处理规范化之前的示例中看到的结果几乎相同。     \n",
    "      \n",
    "**TODO：** 修改`fully_connected`以将批处理规范化添加到它创建的完全连接层。如果有帮助，可以随意更改函数的参数。      \n",
    "            \n",
    "**Note：** 为了方便起见，我们继续使用`tf.layers.dense`作为`fully_connected`层。在类中的这一点上，用`prev_layer`和显式权重和偏差变量之间的矩阵操作替换它应该没有问题。\n",
    "***\n",
    "这种完全连接的实现比使用`tf.layers`的实现要复杂得多。但是，如果你浏览了`Batch_Normalization_Lesson`笔记本，事情看起来应该很熟悉。要添加批处理规范化，我们执行了以下操作：     \n",
    "      \n",
    "1.将`is_training`参数添加到函数签名中，以便我们可以将该信息传递到批处理规范化层。        \n",
    "2.去除了稠密层的bias和activate函数。  \n",
    "3.添加了gamma、beta、pop_mean和pop_variance变量。            \n",
    "4.使用`tf.cond`进行不同的处理训练和推理。         \n",
    "5.训练时，我们使用`tf.nn`矩来计算批均值和方差。然后我们更新总体统计数据，并使用`tf.nn.batch_normalization`来使用批次统计数据规范化层的输出。注意具有`tf.control_dependencies...`语句-这是强制TensorFlow运行更新填充统计信息的操作所必需的。     \n",
    "6.在推断过程中（即不训练时），我们使用`tf.nn.batch_normalization `，使用我们在训练期间计算的总体统计数据来规范化层的输出。       \n",
    "7.将规范化值传递到ReLU激活函数中。      \n",
    "          \n",
    "如果这些代码中的任何一个不清楚，它几乎与我们在`Batch_Normalization_Lesson`笔记本中的`full_connected`函数中显示的内容相同。请看这篇文章以获得广泛的评论。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected(prev_layer, num_units, is_training):\n",
    "    \"\"\"\n",
    "    Create a fully connectd layer with the given layer as input and the given number of neurons.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param num_units: int\n",
    "        The size of the layer. That is, the number of units, nodes, or neurons.\n",
    "    :param is_training: bool or Tensor\n",
    "        Indicates whether or not the network is currently training, which tells the batch normalization\n",
    "        layer whether or not it should update or use its population statistics.\n",
    "    :returns Tensor\n",
    "        A new fully connected layer\n",
    "    \"\"\"\n",
    "\n",
    "    layer = tf.layers.dense(prev_layer, num_units, use_bias=False, activation=None)\n",
    "\n",
    "    gamma = tf.Variable(tf.ones([num_units]))\n",
    "    beta = tf.Variable(tf.zeros([num_units]))\n",
    "\n",
    "    pop_mean = tf.Variable(tf.zeros([num_units]), trainable=False)\n",
    "    pop_variance = tf.Variable(tf.ones([num_units]), trainable=False)\n",
    "\n",
    "    epsilon = 1e-3\n",
    "    \n",
    "    def batch_norm_training():\n",
    "        batch_mean, batch_variance = tf.nn.moments(layer, [0])\n",
    "\n",
    "        decay = 0.99\n",
    "        train_mean = tf.assign(pop_mean, pop_mean * decay + batch_mean * (1 - decay))\n",
    "        train_variance = tf.assign(pop_variance, pop_variance * decay + batch_variance * (1 - decay))\n",
    "\n",
    "        with tf.control_dependencies([train_mean, train_variance]):\n",
    "            return tf.nn.batch_normalization(layer, batch_mean, batch_variance, beta, gamma, epsilon)\n",
    " \n",
    "    def batch_norm_inference():\n",
    "        return tf.nn.batch_normalization(layer, pop_mean, pop_variance, beta, gamma, epsilon)\n",
    "\n",
    "    batch_normalized_output = tf.cond(is_training, batch_norm_training, batch_norm_inference)\n",
    "    return tf.nn.relu(batch_normalized_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO：**修改`conv_layer`，将批处理规范化添加到它创建的完全连接的层。如果有帮助，可以随意更改函数的参数。   \n",
    "    \n",
    "**Note：**与前面使用`tf.layers`的示例不同，向这些卷积层添加批处理规范化确实需要与在`fully_connected`中所做的略有不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(prev_layer, layer_depth, is_training):\n",
    "    \"\"\"\n",
    "    Create a convolutional layer with the given layer as input.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param layer_depth: int\n",
    "        We'll set the strides and number of feature maps based on the layer's depth in the network.\n",
    "        This is *not* a good way to make a CNN, but it helps us create this example with very little code.\n",
    "    :param is_training: bool or Tensor\n",
    "        Indicates whether or not the network is currently training, which tells the batch normalization\n",
    "        layer whether or not it should update or use its population statistics.\n",
    "    :returns Tensor\n",
    "        A new convolutional layer\n",
    "    \"\"\"\n",
    "    strides = 2 if layer_depth % 3 == 0 else 1\n",
    "    \n",
    "    in_channels = prev_layer.get_shape().as_list()[3]\n",
    "    out_channels = layer_depth*4\n",
    "    \n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([3, 3, in_channels, out_channels], stddev=0.05))\n",
    "    \n",
    "    layer = tf.nn.conv2d(prev_layer, weights, strides=[1,strides, strides, 1], padding='SAME')\n",
    "\n",
    "    gamma = tf.Variable(tf.ones([out_channels]))\n",
    "    beta = tf.Variable(tf.zeros([out_channels]))\n",
    "\n",
    "    pop_mean = tf.Variable(tf.zeros([out_channels]), trainable=False)\n",
    "    pop_variance = tf.Variable(tf.ones([out_channels]), trainable=False)\n",
    "\n",
    "    epsilon = 1e-3\n",
    "    \n",
    "    def batch_norm_training():\n",
    "        # Important to use the correct dimensions here to ensure the mean and variance are calculated \n",
    "        # per feature map instead of for the entire layer\n",
    "        batch_mean, batch_variance = tf.nn.moments(layer, [0,1,2], keep_dims=False)\n",
    "\n",
    "        decay = 0.99\n",
    "        train_mean = tf.assign(pop_mean, pop_mean * decay + batch_mean * (1 - decay))\n",
    "        train_variance = tf.assign(pop_variance, pop_variance * decay + batch_variance * (1 - decay))\n",
    "\n",
    "        with tf.control_dependencies([train_mean, train_variance]):\n",
    "            return tf.nn.batch_normalization(layer, batch_mean, batch_variance, beta, gamma, epsilon)\n",
    " \n",
    "    def batch_norm_inference():\n",
    "        return tf.nn.batch_normalization(layer, pop_mean, pop_variance, beta, gamma, epsilon)\n",
    "\n",
    "    batch_normalized_output = tf.cond(is_training, batch_norm_training, batch_norm_inference)\n",
    "    return tf.nn.relu(batch_normalized_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 编辑`train`函数以支持批量规范化。你需要确保网络知道它是否在训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0: Validation loss: 0.69088, Validation accuracy: 0.09900\n",
      "Batch: 25: Training loss: 0.57144, Training accuracy: 0.15625\n",
      "Batch: 50: Training loss: 0.46165, Training accuracy: 0.10938\n",
      "Batch: 75: Training loss: 0.39725, Training accuracy: 0.06250\n",
      "Batch: 100: Validation loss: 0.36217, Validation accuracy: 0.09240\n",
      "Batch: 125: Training loss: 0.34594, Training accuracy: 0.10938\n",
      "Batch: 150: Training loss: 0.34605, Training accuracy: 0.12500\n",
      "Batch: 175: Training loss: 0.36240, Training accuracy: 0.06250\n",
      "Batch: 200: Validation loss: 0.36065, Validation accuracy: 0.09240\n",
      "Batch: 225: Training loss: 0.37682, Training accuracy: 0.14062\n",
      "Batch: 250: Training loss: 0.38396, Training accuracy: 0.10938\n",
      "Batch: 275: Training loss: 0.41749, Training accuracy: 0.04688\n",
      "Batch: 300: Validation loss: 0.39758, Validation accuracy: 0.14540\n",
      "Batch: 325: Training loss: 0.43828, Training accuracy: 0.15625\n",
      "Batch: 350: Training loss: 0.55839, Training accuracy: 0.10938\n",
      "Batch: 375: Training loss: 0.50879, Training accuracy: 0.14062\n",
      "Batch: 400: Validation loss: 0.26779, Validation accuracy: 0.47620\n",
      "Batch: 425: Training loss: 0.37701, Training accuracy: 0.46875\n",
      "Batch: 450: Training loss: 0.15673, Training accuracy: 0.71875\n",
      "Batch: 475: Training loss: 0.14598, Training accuracy: 0.79688\n",
      "Batch: 500: Validation loss: 0.12265, Validation accuracy: 0.82160\n",
      "Batch: 525: Training loss: 0.10882, Training accuracy: 0.78125\n",
      "Batch: 550: Training loss: 0.16898, Training accuracy: 0.76562\n",
      "Batch: 575: Training loss: 0.21170, Training accuracy: 0.71875\n",
      "Batch: 600: Validation loss: 0.03661, Validation accuracy: 0.95140\n",
      "Batch: 625: Training loss: 0.04453, Training accuracy: 0.89062\n",
      "Batch: 650: Training loss: 0.00803, Training accuracy: 0.98438\n",
      "Batch: 675: Training loss: 0.05712, Training accuracy: 0.89062\n",
      "Batch: 700: Validation loss: 0.05395, Validation accuracy: 0.93660\n",
      "Batch: 725: Training loss: 0.04642, Training accuracy: 0.92188\n",
      "Batch: 750: Training loss: 0.05393, Training accuracy: 0.90625\n",
      "Batch: 775: Training loss: 0.00873, Training accuracy: 0.98438\n",
      "Final validation accuracy: 0.89640\n",
      "Final test accuracy: 0.90130\n",
      "Accuracy on 100 samples: 0.89\n"
     ]
    }
   ],
   "source": [
    "def train(num_batches, batch_size, learning_rate):\n",
    "    # Build placeholders for the input samples and labels \n",
    "    inputs = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    labels = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "    # Add placeholder to indicate whether or not we're training the model\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "    # Feed the inputs into a series of 20 convolutional layers \n",
    "    layer = inputs\n",
    "    for layer_i in range(1, 20):\n",
    "        layer = conv_layer(layer, layer_i, is_training)\n",
    "\n",
    "    # Flatten the output from the convolutional layers \n",
    "    orig_shape = layer.get_shape().as_list()\n",
    "    layer = tf.reshape(layer, shape=[-1, orig_shape[1] * orig_shape[2] * orig_shape[3]])\n",
    "\n",
    "    # Add one fully connected layer\n",
    "    layer = fully_connected(layer, 100, is_training)\n",
    "\n",
    "    # Create the output layer with 1 node for each \n",
    "    logits = tf.layers.dense(layer, 10)\n",
    "    \n",
    "    # Define loss and training operations\n",
    "    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    train_opt = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "    \n",
    "    # Create operations to test accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # Train and test the network\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for batch_i in range(num_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # train this batch\n",
    "            sess.run(train_opt, {inputs: batch_xs, labels: batch_ys, is_training: True})\n",
    "            \n",
    "            # Periodically check the validation or training loss and accuracy\n",
    "            if batch_i % 100 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: mnist.validation.images,\n",
    "                                                              labels: mnist.validation.labels,\n",
    "                                                              is_training: False})\n",
    "                print('Batch: {:>2}: Validation loss: {:>3.5f}, Validation accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "            elif batch_i % 25 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: batch_xs, labels: batch_ys, is_training: False})\n",
    "                print('Batch: {:>2}: Training loss: {:>3.5f}, Training accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "\n",
    "        # At the end, score the final accuracy for both the validation and test sets\n",
    "        acc = sess.run(accuracy, {inputs: mnist.validation.images,\n",
    "                                  labels: mnist.validation.labels, \n",
    "                                  is_training: False})\n",
    "        print('Final validation accuracy: {:>3.5f}'.format(acc))\n",
    "        acc = sess.run(accuracy, {inputs: mnist.test.images,\n",
    "                                  labels: mnist.test.labels,\n",
    "                                  is_training: False})\n",
    "        print('Final test accuracy: {:>3.5f}'.format(acc))\n",
    "        \n",
    "        # Score the first 100 test images individually, just to make sure batch normalization really worked\n",
    "        correct = 0\n",
    "        for i in range(100):\n",
    "            correct += sess.run(accuracy,feed_dict={inputs: [mnist.test.images[i]],\n",
    "                                                    labels: [mnist.test.labels[i]],\n",
    "                                                    is_training: False})\n",
    "\n",
    "        print(\"Accuracy on 100 samples:\", correct/100)\n",
    "\n",
    "\n",
    "num_batches = 800\n",
    "batch_size = 64\n",
    "learning_rate = 0.002\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    train(num_batches, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再次，批量标准化模型的精度应达到90%以上。在这个低层次上实现时，有很多细节可能会出错，所以如果你成功了-干得好！如果没有，不用担心，只要看看`Batch_Normalization_Solutions`笔记本，看看哪里出了问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\;$$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
